{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP9v495vbnud86EjYK6nR3P"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#**Theory** **Questions**"],"metadata":{"id":"3vnZYpXj9e-m"}},{"cell_type":"markdown","source":["1. What is a Support Vector Machine (SVM)?\n","\n","Support Vector Machine (SVM) is a supervised learning algorithm used for classification and regression tasks. It finds the optimal hyperplane that best separates different classes in a dataset. The goal is to maximize the margin between the closest points (support vectors) of different classes.\n","\n","2. What is the difference between Hard Margin and Soft Margin SVM?\n","\n","Hard Margin SVM:\n","\n","Used when data is linearly separable.\n","\n","No misclassification is allowed.\n","\n","It may lead to overfitting if the data has noise.\n","\n","Soft Margin SVM:\n","\n","Used when data is not perfectly separable.\n","\n","Allows some misclassification using a penalty parameter C.\n","\n","Provides better generalization.\n","\n","3. What is the mathematical intuition behind SVM?\n","\n","SVM tries to solve the following optimization problem:\n","min\n","​\n","  \n","2\n","1\n","​\n"," ∣∣w∣∣\n","2\n","\n","Subject to the constraint:\n","\n","𝑦\n","𝑖\n","(\n","𝑤\n","⋅\n","𝑥\n","𝑖\n","+\n","𝑏\n",")\n","≥\n","1\n","y\n","i\n","​\n"," (w⋅x\n","i\n","​\n"," +b)≥1\n","for all i, where:\n","\n","𝑤\n","w is the weight vector,\n","\n","𝑏\n","b is the bias term,\n","\n","𝑦\n","𝑖\n","y\n","i\n","​\n","  is the class label (\n","+\n","1\n","+1 or\n","−\n","1\n","−1),\n","\n","𝑥\n","𝑖\n","x\n","i\n","​\n","  is the feature vector.\n","\n","The objective is to maximize the margin (distance between the hyperplane and the nearest data points).\n","\n","\n","The objective is to maximize the margin (distance between the hyperplane and the nearest data points).\n","\n","4. What is the role of Lagrange Multipliers in SVM?\n","\n","Lagrange multipliers help transform the constrained optimization problem into an unconstrained one using the Lagrangian function:\n","\n","𝐿\n","(\n","𝑤\n",",\n","𝑏\n",",\n","𝜆\n",")\n","=\n","1\n","2\n","∣\n","∣\n","𝑤\n","∣\n","∣\n","2\n","−\n","∑\n","𝑖\n","=\n","1\n","𝑛\n","𝜆\n","𝑖\n","(\n","𝑦\n","𝑖\n","(\n","𝑤\n","⋅\n","𝑥\n","𝑖\n","+\n","𝑏\n",")\n","−\n","1\n",")\n","L(w,b,λ)=\n","2\n","1\n","​\n"," ∣∣w∣∣\n","2\n"," −\n","i=1\n","∑\n","n\n","​\n"," λ\n","i\n","​\n"," (y\n","i\n","​\n"," (w⋅x\n","i\n","​\n"," +b)−1)\n","This allows solving the problem using Quadratic Programming.\n","\n","5. What are Support Vectors in SVM?\n","\n","Support vectors are the data points that lie closest to the hyperplane. These points define the margin and directly affect the decision boundary. Removing them changes the hyperplane, making them crucial in classification.\n","\n","6. What is a Support Vector Classifier (SVC)?\n","\n","SVC is an SVM model used specifically for classification tasks. It finds an optimal hyperplane to separate classes while maximizing the margin.\n","\n","7. What is a Support Vector Regressor (SVR)?\n","\n","SVR is the regression version of SVM that tries to fit a function within a given margin ε (epsilon). It minimizes the error outside this margin rather than focusing on exact predictions.\n","\n","8. What is the Kernel Trick in SVM?\n","\n","The Kernel Trick allows SVM to work in non-linearly separable data by mapping it into a higher-dimensional space without explicitly computing transformations. This makes SVM more powerful for complex datasets.\n","\n","9. Compare Linear Kernel, Polynomial Kernel, and RBF Kernel.\n","Kernel Type\tFormula\tWhen to Use?\n","\n","Linear\n","𝐾\n","(\n","𝑥\n",",\n","𝑦\n",")\n","=\n","𝑥\n","⋅\n","𝑦\n","K(x,y)=x⋅y\tWhen data is linearly separable\n","Polynomial\n","𝐾\n","(\n","𝑥\n",",\n","𝑦\n",")\n","=\n","(\n","𝑥\n","⋅\n","𝑦\n","+\n","𝑐\n",")\n","𝑑\n","K(x,y)=(x⋅y+c)\n","d\n"," \tWhen data has some curvature\n","RBF (Gaussian)\t( K(x, y) = \\exp(-\\gamma\n","10. What is the effect of the C parameter in SVM?\n","\n","High C → Less margin, fewer misclassifications (Overfitting)\n","\n","Low C → Larger margin, more misclassifications (Better generalization)\n","\n","11. What is the role of the Gamma parameter in RBF Kernel SVM?\n","\n","Gamma (\n","𝛾\n","γ) defines how much influence a single training point has.\n","\n","High Gamma → Model focuses on fewer points, can lead to overfitting.\n","\n","Low Gamma → Model considers more points, can lead to underfitting.\n","\n","12. What is the Naïve Bayes classifier, and why is it called \"Naïve\"?\n","\n","Naïve Bayes is a probabilistic classifier based on Bayes’ Theorem. It assumes features are independent, which is rarely true in real-world data, making it a \"naïve\" assumption.\n","\n","13. What is Bayes’ Theorem?\n","\n","Bayes’ Theorem states:\n","\n","𝑃\n","(\n","𝐴\n","∣\n","𝐵\n",")\n","=\n","𝑃\n","(\n","𝐵\n","∣\n","𝐴\n",")\n","𝑃\n","(\n","𝐴\n",")\n","𝑃\n","(\n","𝐵\n",")\n","P(A∣B)=\n","P(B)\n","P(B∣A)P(A)\n","​\n","\n","where:\n","\n","𝑃\n","(\n","𝐴\n","∣\n","𝐵\n",")\n","P(A∣B) = Probability of A given B\n","\n","𝑃\n","(\n","𝐵\n","∣\n","𝐴\n",")\n","P(B∣A) = Probability of B given A\n","\n","𝑃\n","(\n","𝐴\n",")\n","P(A) = Prior probability of A\n","\n","𝑃\n","(\n","𝐵\n",")\n","P(B) = Prior probability of B\n","\n","14. Differences between Gaussian, Multinomial, and Bernoulli Naïve Bayes?\n","Type\tWhen to Use?\n","\tFormula\n","Gaussian\tContinuous data\tUses normal distribution\n","Multinomial\tText data (word counts)\tUses term frequency\n","Bernoulli\tBinary features\tUses binary occurrence (0/1)\n","15. When should you use Gaussian Naïve Bayes over other variants?\n","\n","Use Gaussian Naïve Bayes when working with continuous numerical features.\n","\n","16. What are the key assumptions made by Naïve Bayes?\n","\n","Feature Independence\n","\n","Equal Importance of Features\n","\n","Conditional Probability is Fixed\n","\n","17. What are the advantages and disadvantages of Naïve Bayes?\n","\n","✅ Advantages:\n","\n","Fast and efficient\n","\n","Works well with small datasets\n","\n","Handles text classification well\n","\n","❌ Disadvantages:\n","\n","Assumes feature independence (unrealistic)\n","\n","Struggles with correlated features\n","\n","18. Why is Naïve Bayes a good choice for text classification?\n","\n","Works well with high-dimensional data\n","\n","Efficient in handling sparse data (text has many zero values)\n","\n","Uses word frequency effectively\n","\n","19. Compare SVM and Naïve Bayes for classification tasks.\n","\n","Feature\tSVM\tNaïve Bayes\n","Data Type\tWorks well with numeric data\tWorks well with text\n","Training Time\tSlow\tFast\n","Assumptions\tNo assumptions about data\tAssumes feature independence\n","Best For\tLarge datasets with non-linear relationships\tText classification\n","\n","20. How does Laplace Smoothing help in Naïve Bayes?\n","\n","Laplace Smoothing prevents zero probability issues when encountering new words by adding a small constant\n","𝛼\n","α to all probabilities:\n","\n","𝑃\n","(\n","𝑤\n","∣\n","𝑐\n",")\n","=\n","𝑐\n","𝑜\n","𝑢\n","𝑛\n","𝑡\n","(\n","𝑤\n",",\n","𝑐\n",")\n","+\n","𝛼\n","𝑐\n","𝑜\n","𝑢\n","𝑛\n","𝑡\n","(\n","𝑐\n",")\n","+\n","𝛼\n","⋅\n","𝑉\n","P(w∣c)=\n","count(c)+α⋅V\n","count(w,c)+α\n","​\n","\n","where V is the vocabulary size."],"metadata":{"id":"vHj9r73e9mKj"}},{"cell_type":"markdown","source":["#**Practical** **Questions**"],"metadata":{"id":"t5lNULsjBE4b"}},{"cell_type":"markdown","source":["21. Write a Python program to train an SVM Classifier on the Iris dataset and evaluate accuracy:"],"metadata":{"id":"qrScqpjRBR3m"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"S1y0kdt69eEW"},"outputs":[],"source":["# Import necessary libraries\n","from sklearn import datasets\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.metrics import accuracy_score\n","\n","# Load the Iris dataset\n","iris = datasets.load_iris()\n","X, y = iris.data, iris.target\n","\n","# Split data into training and testing sets (80% train, 20% test)\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train SVC model\n","svm_model = SVC(kernel='linear', C=1.0)\n","svm_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = svm_model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Support Vector Classifier Accuracy: {accuracy:.2f}\")\n"]},{"cell_type":"markdown","source":["22. Write a Python program to train two SVM classifiers with Linear and RBF kernels on the Wine dataset, then\n","compare their accuracies.\n"],"metadata":{"id":"8hB0mlNeBVCK"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.svm import SVR\n","from sklearn.datasets import fetch_california_housing\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import mean_absolute_error\n","\n","# Load the California housing dataset (since Boston is deprecated)\n","housing = fetch_california_housing()\n","X, y = housing.data, housing.target\n","\n","# Standardize the dataset\n","scaler = StandardScaler()\n","X = scaler.fit_transform(X)\n","\n","# Split data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train SVR model\n","svr_model = SVR(kernel='rbf', C=100, gamma=0.1)\n","svr_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = svr_model.predict(X_test)\n","\n","# Evaluate model performance\n","mae = mean_absolute_error(y_test, y_pred)\n","print(f\"Support Vector Regressor MAE: {mae:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"HLaeAIgrD4Nd","executionInfo":{"status":"error","timestamp":1743658325912,"user_tz":-330,"elapsed":17847,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"3062487d-e5d5-4742-867f-1f821c898b56"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_test_split' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-ddb69d56bee8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Split data into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create and train SVR model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"]}]},{"cell_type":"markdown","source":["23. Write a Python program to train an SVM Regressor (SVR) on a housing dataset and evaluate it using Mean\n","Squared Error (MSE):"],"metadata":{"id":"EfUnb2fnD4vn"}},{"cell_type":"code","source":["# Import necessary libraries\n","import pandas as pd\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.impute import SimpleImputer\n","\n","# Load the Titanic dataset\n","url = \"https://raw.githubusercontent.com/datasciencedojo/datasets/master/titanic.csv\"\n","df = pd.read_csv(url)\n","\n","# Select relevant features\n","df = df[['Pclass', 'Sex', 'Age', 'Fare', 'Survived']]\n","\n","# Handle missing values (replace missing Age with mean)\n","imputer = SimpleImputer(strategy=\"mean\")\n","df['Age'] = imputer.fit_transform(df[['Age']])\n","\n","# Convert categorical feature 'Sex' to numeric\n","encoder = LabelEncoder()\n","df['Sex'] = encoder.fit_transform(df['Sex'])  # 0: female, 1: male\n","\n","# Split data into features (X) and target (y)\n","X = df[['Pclass', 'Sex', 'Age', 'Fare']]\n","y = df['Survived']\n","\n","# Split into training and test sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train Naïve Bayes classifier\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = nb_model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Naïve Bayes Classifier Accuracy on Titanic Dataset: {accuracy:.2f}\")\n"],"metadata":{"id":"kwd7Zv-4D9jB","executionInfo":{"status":"aborted","timestamp":1743658325928,"user_tz":-330,"elapsed":2,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["24. Write a Python program to train an SVM Classifier with a Polynomial Kernel and visualize the decision\n","boundary.\n"],"metadata":{"id":"zZkPfxbeD-E1"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.datasets import fetch_20newsgroups\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.naive_bayes import MultinomialNB\n","\n","# Load the 20 Newsgroups dataset\n","categories = ['rec.sport.baseball', 'sci.space', 'comp.graphics', 'talk.politics.mideast']\n","newsgroups = fetch_20newsgroups(subset='train', categories=categories, remove=('headers', 'footers', 'quotes'))\n","\n","# Convert text data to numerical vectors\n","vectorizer = CountVectorizer()\n","X = vectorizer.fit_transform(newsgroups.data)\n","y = newsgroups.target\n","\n","# Split into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n","\n","# Create and train Multinomial Naïve Bayes model\n","mnb_model = MultinomialNB()\n","mnb_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","y_pred = mnb_model.predict(X_test)\n","\n","# Evaluate accuracy\n","accuracy = accuracy_score(y_test, y_pred)\n","print(f\"Multinomial Naïve Bayes Accuracy on 20 Newsgroups: {accuracy:.2f}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"bpZPGjD4EOS_","executionInfo":{"status":"error","timestamp":1743658371008,"user_tz":-330,"elapsed":43498,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"2e0dd47f-f669-4a6c-e4c6-043423e693ef"},"execution_count":2,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'train_test_split' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-2-5316d044acf3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m# Split into training and testing sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Create and train Multinomial Naïve Bayes model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'train_test_split' is not defined"]}]},{"cell_type":"markdown","source":["25. Write a Python program to train a Gaussian Naïve Bayes classifier on the Breast Cancer dataset and\n","evaluate accuracy:"],"metadata":{"id":"YKWyKCmoEOx4"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.naive_bayes import BernoulliNB\n","from sklearn.pipeline import Pipeline\n","from sklearn.svm import SVC\n","from sklearn.model_selection import cross_val_score\n","from sklearn.datasets import fetch_openml\n","\n","# Load the Spam Email dataset\n","spam_data = fetch_openml(\"spam\", version=1, as_frame=True)\n","X, y = spam_data.data, spam_data.target.astype('int')  # Convert target to integer\n","\n","# Define models\n","svm_model = SVC(kernel='linear', C=1.0)\n","nb_model = BernoulliNB()\n","\n","# Perform cross-validation and compare accuracy\n","svm_accuracy = cross_val_score(svm_model, X, y, cv=5).mean()\n","nb_accuracy = cross_val_score(nb_model, X, y, cv=5).mean()\n","\n","print(f\"SVM Accuracy on Spam Dataset: {svm_accuracy:.2f}\")\n","print(f\"Naïve Bayes Accuracy on Spam Dataset: {nb_accuracy:.2f}\")\n"],"metadata":{"id":"b2hpxLAmESOr","executionInfo":{"status":"aborted","timestamp":1743658371018,"user_tz":-330,"elapsed":12,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["26. Write a Python program to train a Multinomial Naïve Bayes classifier for text classification using the 20\n","Newsgroups dataset."],"metadata":{"id":"R8kEcDitEUgc"}},{"cell_type":"code","source":["# Import necessary libraries\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA\n","\n","# Apply PCA to reduce dimensions to 2\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","\n","# Scatter plot of PCA results\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', alpha=0.7)\n","plt.xlabel(\"Principal Component 1\")\n","plt.ylabel(\"Principal Component 2\")\n","plt.title(\"PCA on Iris Dataset\")\n","plt.colorbar(label=\"Target Classes\")\n","plt.show()\n"],"metadata":{"id":"SYVNdS7FEYFG","executionInfo":{"status":"aborted","timestamp":1743658371020,"user_tz":-330,"elapsed":13,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["27. Write a Python program to train an SVM Classifier with different C values and compare the decision\n","boundaries visually."],"metadata":{"id":"ltSYFhXGEYwz"}},{"cell_type":"code","source":["# Import necessary libraries\n","from sklearn.cluster import KMeans\n","\n","# Apply k-Means clustering\n","kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)\n","clusters = kmeans.fit_predict(X)\n","\n","# Scatter plot of k-Means clusters\n","plt.figure(figsize=(8, 6))\n","plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='rainbow', edgecolor='k', alpha=0.7)\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, c='black', marker='X', label='Centroids')\n","plt.xlabel(\"Feature 1\")\n","plt.ylabel(\"Feature 2\")\n","plt.title(\"k-Means Clustering on Iris Dataset\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"3yfDaYi1EcvM","executionInfo":{"status":"aborted","timestamp":1743658371039,"user_tz":-330,"elapsed":17,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["28. Write a Python program to train a Bernoulli Naïve Bayes classifier for binary classification on a dataset with\n","binary features."],"metadata":{"id":"PuESzv0IEeE2"}},{"cell_type":"code","source":["# Import Decision Tree Classifier\n","from sklearn.tree import DecisionTreeClassifier, plot_tree\n","\n","# Create and train Decision Tree model\n","dt_model = DecisionTreeClassifier(max_depth=3, random_state=42)\n","dt_model.fit(X_train, y_train)\n","\n","# Plot the Decision Tree\n","plt.figure(figsize=(12, 6))\n","plot_tree(dt_model, feature_names=['Pclass', 'Sex', 'Age', 'Fare'], class_names=['Not Survived', 'Survived'], filled=True)\n","plt.show()\n"],"metadata":{"id":"4WofB2WbEhgu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["29. Write a Python program to apply feature scaling before training an SVM model and compare results with\n","unscaled data."],"metadata":{"id":"QPa6-E4YEiAp"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Create and train Random Forest model\n","rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","rf_pred = rf_model.predict(X_test)\n","\n","# Compare accuracies\n","dt_accuracy = accuracy_score(y_test, dt_model.predict(X_test))\n","rf_accuracy = accuracy_score(y_test, rf_pred)\n","\n","print(f\"Decision Tree Accuracy: {dt_accuracy:.2f}\")\n","print(f\"Random Forest Accuracy: {rf_accuracy:.2f}\")\n"],"metadata":{"id":"KurPkzh3EnII"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["30. Write a Python program to train a Gaussian Naïve Bayes model and compare the predictions before and\n","after Laplace Smoothing."],"metadata":{"id":"-Y6kDCzwEnqt"}},{"cell_type":"code","source":["from sklearn.linear_model import LogisticRegression\n","\n","# Create and train Logistic Regression model\n","logreg_model = LogisticRegression()\n","logreg_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","logreg_pred = logreg_model.predict(X_test)\n","\n","# Compare accuracy\n","logreg_accuracy = accuracy_score(y_test, logreg_pred)\n","\n","print(f\"Logistic Regression Accuracy: {logreg_accuracy:.2f}\")\n","print(f\"Random Forest Accuracy: {rf_accuracy:.2f}\")\n"],"metadata":{"id":"bsLra0hqErT3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["31. Write a Python program to train an SVM Classifier and use GridSearchCV to tune the hyperparameters (C,\n","gamma, kernel)."],"metadata":{"id":"fXSUgyt0Er93"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsClassifier\n","\n","# Create and train k-NN model\n","knn_model = KNeighborsClassifier(n_neighbors=5)\n","knn_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","knn_pred = knn_model.predict(X_test)\n","\n","# Evaluate accuracy\n","knn_accuracy = accuracy_score(y_test, knn_pred)\n","print(f\"k-NN Classifier Accuracy: {knn_accuracy:.2f}\")\n"],"metadata":{"id":"6vauHC7nEveQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["32. Write a Python program to train an SVM Classifier on an imbalanced dataset and apply class weighting and\n","check it improve accuracy."],"metadata":{"id":"19n_fcCYEv6X"}},{"cell_type":"code","source":["from sklearn.linear_model import LinearRegression\n","from sklearn.metrics import mean_squared_error\n","\n","# Create and train Linear Regression model\n","linreg_model = LinearRegression()\n","linreg_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","linreg_pred = linreg_model.predict(X_test)\n","\n","# Evaluate performance\n","mse = mean_squared_error(y_test, linreg_pred)\n","print(f\"Linear Regression Mean Squared Error: {mse:.2f}\")\n"],"metadata":{"id":"lcnNgnZKEzPs"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["33. Write a Python program to implement a Naïve Bayes classifier for spam detection using email data."],"metadata":{"id":"KZNC4i6eEzzU"}},{"cell_type":"code","source":["from sklearn.linear_model import Ridge, Lasso\n","\n","# Create and train Ridge model\n","ridge_model = Ridge(alpha=1.0)\n","ridge_model.fit(X_train, y_train)\n","ridge_pred = ridge_model.predict(X_test)\n","\n","# Create and train Lasso model\n","lasso_model = Lasso(alpha=0.1)\n","lasso_model.fit(X_train, y_train)\n","lasso_pred = lasso_model.predict(X_test)\n","\n","# Evaluate performance\n","ridge_mse = mean_squared_error(y_test, ridge_pred)\n","lasso_mse = mean_squared_error(y_test, lasso_pred)\n","\n","print(f\"Ridge Regression MSE: {ridge_mse:.2f}\")\n","print(f\"Lasso Regression MSE: {lasso_mse:.2f}\")\n"],"metadata":{"id":"QQpFnK1gE4mU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["34. Write a Python program to train an SVM Classifier and a Naïve Bayes Classifier on the same dataset and\n","compare their accuracy."],"metadata":{"id":"v4166cgeE6pD"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","\n","# Create and train XGBoost model\n","xgb_model = XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n","xgb_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","xgb_pred = xgb_model.predict(X_test)\n","\n","# Evaluate accuracy\n","xgb_accuracy = accuracy_score(y_test, xgb_pred)\n","print(f\"XGBoost Accuracy: {xgb_accuracy:.2f}\")\n"],"metadata":{"id":"NUEY5OMSE-Ab"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["35. Write a Python program to perform feature selection before training a Naïve Bayes classifier and compare\n","results."],"metadata":{"id":"bfRazxfwE-dZ"}},{"cell_type":"code","source":["from sklearn.svm import SVR\n","\n","# Create and train SVR model\n","svr_model = SVR(kernel='linear')\n","svr_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","svr_pred = svr_model.predict(X_test)\n","\n","# Evaluate performance\n","svr_mae = mean_absolute_error(y_test, svr_pred)\n","print(f\"SVR Mean Absolute Error: {svr_mae:.2f}\")\n"],"metadata":{"id":"64pXr-U3FB10"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["36. Write a Python program to train an SVM Classifier using One-vs-Rest (OvR) and One-vs-One (OvO)\n","strategies on the Wine dataset and compare their accuracy."],"metadata":{"id":"APe789HRFCVY"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","# Create and train Naïve Bayes model\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","nb_pred = nb_model.predict(X_test)\n","\n","# Evaluate accuracy\n","nb_accuracy = accuracy_score(y_test, nb_pred)\n","print(f\"Naïve Bayes Accuracy: {nb_accuracy:.2f}\")\n"],"metadata":{"id":"OOF3vFG7FGJx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["37. Write a Python program to train an SVM Classifier using Linear, Polynomial, and RBF kernels on the Breast\n","Cancer dataset and compare their accuracy."],"metadata":{"id":"BYV3ujQNFGp6"}},{"cell_type":"code","source":["from sklearn.naive_bayes import GaussianNB\n","\n","# Create and train Naïve Bayes model\n","nb_model = GaussianNB()\n","nb_model.fit(X_train, y_train)\n","\n","# Predict on test data\n","nb_pred = nb_model.predict(X_test)\n","\n","# Evaluate accuracy\n","nb_accuracy = accuracy_score(y_test, nb_pred)\n","print(f\"Naïve Bayes Accuracy: {nb_accuracy:.2f}\")\n"],"metadata":{"id":"DiNQjI1tFL4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["38. Write a Python program to train an SVM Classifier using Stratified K-Fold Cross-Validation and compute the\n","average accuracy."],"metadata":{"id":"occ5yf6UFMWm"}},{"cell_type":"code","source":["from sklearn.naive_bayes import MultinomialNB\n","from sklearn.feature_extraction.text import CountVectorizer\n","\n","# Sample text data\n","texts = [\"I love coding\", \"Python is great\", \"Machine learning is amazing\", \"Deep learning is powerful\"]\n","labels = [1, 1, 0, 0]  # 1 = Positive, 0 = Negative\n","\n","# Convert text to numerical format\n","vectorizer = CountVectorizer()\n","X_text = vectorizer.fit_transform(texts)\n","\n","# Train Multinomial Naïve Bayes\n","mnb_model = MultinomialNB()\n","mnb_model.fit(X_text, labels)\n","\n","# Predict on new text\n","new_texts = [\"I enjoy Python\", \"AI is the future\"]\n","X_new = vectorizer.transform(new_texts)\n","predictions = mnb_model.predict(X_new)\n","\n","print(f\"Predictions: {predictions}\")\n"],"metadata":{"id":"aQ88KnzKFQ4L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["39. Write a Python program to train a Naïve Bayes classifier using different prior probabilities and compare\n","performance."],"metadata":{"id":"2-r9TyqtFRXC"}},{"cell_type":"code","source":["import scipy.cluster.hierarchy as sch\n","\n","# Perform hierarchical clustering\n","plt.figure(figsize=(10, 6))\n","dendrogram = sch.dendrogram(sch.linkage(X, method='ward'))\n","plt.title(\"Dendrogram for Hierarchical Clustering\")\n","plt.xlabel(\"Data Points\")\n","plt.ylabel(\"Euclidean Distance\")\n","plt.show()\n"],"metadata":{"id":"tsdav9vBFYqx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["40. Write a Python program to perform Recursive Feature Elimination (RFE) before training an SVM Classifier and\n","compare accuracy."],"metadata":{"id":"IcZzYeYUFb2j"}},{"cell_type":"code","source":["from sklearn.neighbors import KNeighborsRegressor\n","\n","# Create and train k-NN Regressor\n","knn_regressor = KNeighborsRegressor(n_neighbors=5)\n","knn_regressor.fit(X_train, y_train)\n","\n","# Predict on test data\n","knn_reg_pred = knn_regressor.predict(X_test)\n","\n","# Evaluate performance\n","knn_reg_mae = mean_absolute_error(y_test, knn_reg_pred)\n","print(f\"k-NN Regressor MAE: {knn_reg_mae:.2f}\")\n"],"metadata":{"id":"lTmSKzB8FkAL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["41. Write a Python program to train an SVM Classifier and evaluate its performance using Precision, Recall, and\n","F1-Score instead of accuracy."],"metadata":{"id":"3L3Rk1-uFkhG"}},{"cell_type":"code","source":["from sklearn.decomposition import PCA\n","\n","# Apply PCA (reduce to 2 principal components)\n","pca = PCA(n_components=2)\n","X_pca = pca.fit_transform(X)\n","\n","# Visualize the transformed data\n","plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')\n","plt.xlabel('Principal Component 1')\n","plt.ylabel('Principal Component 2')\n","plt.title('PCA on Iris Dataset')\n","plt.show()\n"],"metadata":{"id":"gRhssGdWFphx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["42. Write a Python program to train a Naïve Bayes Classifier and evaluate its performance using Log Loss\n","(Cross-Entropy Loss)."],"metadata":{"id":"9F8kIzMMFqCF"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","# Apply K-Means with 3 clusters\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","clusters = kmeans.fit_predict(X)\n","\n","# Visualize clusters\n","plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='rainbow')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, marker='X', color='black', label=\"Centroids\")\n","plt.title(\"K-Means Clustering on Mall Customers\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"id":"NwCG6hj3FuuV"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["43. Write a Python program to train an SVM Classifier and visualize the Confusion Matrix using seaborn."],"metadata":{"id":"X8GixAitFvLx"}},{"cell_type":"code","source":["from sklearn.tree import DecisionTreeRegressor\n","\n","# Train Decision Tree Regressor\n","dt_regressor = DecisionTreeRegressor()\n","dt_regressor.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","dt_reg_pred = dt_regressor.predict(X_test)\n","dt_reg_mae = mean_absolute_error(y_test, dt_reg_pred)\n","print(f\"Decision Tree Regressor MAE: {dt_reg_mae:.2f}\")\n"],"metadata":{"id":"1jIyFMHbF1Gi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["44. Write a Python program to train an SVM Regressor (SVR) and evaluate its performance using Mean Absolute\n","Error (MAE) instead of MSE."],"metadata":{"id":"L5nY9QMDF1jt"}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n","\n","# Train Random Forest Classifier\n","rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n","rf_classifier.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","rf_pred = rf_classifier.predict(X_test)\n","rf_accuracy = accuracy_score(y_test, rf_pred)\n","print(f\"Random Forest Classifier Accuracy: {rf_accuracy:.2f}\")\n"],"metadata":{"id":"6cr2x0ouF467"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["45. Write a Python program to train a Naïve Bayes classifier and evaluate its performance using the ROC-AUC\n","score."],"metadata":{"id":"DoCumyPxF5pS"}},{"cell_type":"code","source":["from xgboost import XGBClassifier\n","\n","# Train XGBoost Classifier\n","xgb_model = XGBClassifier()\n","xgb_model.fit(X_train, y_train)\n","\n","# Predict and evaluate\n","xgb_pred = xgb_model.predict(X_test)\n","xgb_accuracy = accuracy_score(y_test, xgb_pred)\n","print(f\"XGBoost Classifier Accuracy: {xgb_accuracy:.2f}\")\n"],"metadata":{"id":"JaGoAsYWF8Vw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["46.Write a Python program to train an SVM Classifier and visualize the Precision-Recall Curve."],"metadata":{"id":"vChYs49eF9HZ"}},{"cell_type":"code","source":["from sklearn.cluster import KMeans\n","\n","# Apply K-Means with 3 clusters\n","kmeans = KMeans(n_clusters=3, random_state=42)\n","clusters = kmeans.fit_predict(X)\n","\n","# Visualize clusters\n","plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='rainbow')\n","plt.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], s=200, marker='X', color='black', label=\"Centroids\")\n","plt.title(\"K-Means Clustering on Mall Customers\")\n","plt.legend()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"uou68WJqGCoZ","executionInfo":{"status":"error","timestamp":1743658683330,"user_tz":-330,"elapsed":680,"user":{"displayName":"Shruti Gupta","userId":"00406294573104776226"}},"outputId":"1c4dc18a-316e-464e-f2e5-be10bdc2034c"},"execution_count":3,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'plt' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-3-2c1e1085c613>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# Visualize clusters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclusters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rainbow'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscatter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_centers_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmarker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'X'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'black'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Centroids\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"K-Means Clustering on Mall Customers\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"]}]},{"cell_type":"code","source":[],"metadata":{"id":"WuOrGCkOKAgs"},"execution_count":null,"outputs":[]}]}